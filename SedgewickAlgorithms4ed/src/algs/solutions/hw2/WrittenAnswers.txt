Name: George Heineman / HW2 solutions

1. Sorting Experiment

Here is the output from SortComparison. As you can see, from a timing perspective, MergeSort
and the two QuickSort variations complete too quickly. Note that we have some results for 
Insertion Sort and Selection Sort.

Time Trials
N		Insert		Select		Merge Quick	QuickA

4		0.0			0.0			0.0	0.0	0.0
8		0.0			0.0			0.0	0.0	0.0
16		0.0			0.0			0.0	0.0	0.0
32		0.0			0.0			0.0	0.0	0.0
64		0.0			0.0			0.0	0.0	0.0
128		0.0			0.0			0.0	0.0	0.0
256		0.0			0.0			0.0	0.0	0.0
512		0.0			0.0			0.0	0.0	0.0
1024	0.0			0.0			0.0	0.0	0.0
2048	0.0156001	0.0156001	0.0	0.0	0.0
4096	0.0468003	0.0624004	0.0	0.0	0.0
8192	0.2340015	0.2496016	0.0	0.0	0.0

For Insertion and Selection sort, as the problem size doubles, it looks like the time is multiplied 
by four, which gives an indication that the Order of Growth will be something like ~ N^2

Exch Results
N		Insert	 Select	 Merge	Quick	QuickA

4		3			4		0	2		3
8		4			8		0	6		8
16		48			16		0	15		26
32		205			32		0	39		72
64		895			64		0	85		169
128		3660		128		0	210		414
256		15572		256		0	483		1102
512		59732		512		0	1108	2247
1024	254226		1024	0	2377	5621
2048	1019819		2048	0	5258	12075
4096	4065523		4096	0	11615	27645
8192	16629558	8192	0	25180	61156

First observe that Merge has no official exchanges; however, because of the many data copies, we
could have tried to account for those in that fashion. Clearly Insertion Sort goes crazy with all
of the comparisons (in fact they quadruple as the problem size doubles) and you can see why it is 
so costly; Note how Selection Sort only makes N exchanges, which makes sense, given that it is the
algorithm that searches for the proper one to swap, and only makes one exchange with each pass.
Quicksort makes orders of magnitude fewer exchanges tha Insertion Sort, but the real test will come
with the number of comparisons.

Less Results
N		Insert	Select		Merge	Quick	QuickA

4		5			6			4		7		6
8		10			28			14		22		19
16		61			120			43		64		53
32		234			496			119		152		139
64		953			2016		298		383		380
128		3783		8128		731		912		853
256		15824		32640		1720	2225	2078
512		60240		130816		3936	5101	4658
1024	255241		523776		8907	11552	10937
2048	1021858		2096128		19913	25944	25230
4096	4069613		8386560		43944	58529	56608
8192	16637739	33550336	96057	126726	122276

Here the real benefit of the advanced sorting algorithms is evident. MergeSort makes the fewest number
of comparisons, but it looks like it about "doubles" as the problem size doubles. But it does more than
double. If you look at the RATIO of the final three columns, it will look like this:

 M		 Q	     QA
 3.50 	 3.14 	 3.17 
 3.07 	 2.91 	 2.79 
 2.77 	 2.38 	 2.62 
 2.50 	 2.52 	 2.73 
 2.45 	 2.38 	 2.24 
 2.35 	 2.44 	 2.44 
 2.29 	 2.29 	 2.24 
 2.26 	 2.26 	 2.35 
 2.24 	 2.25 	 2.31 
 2.21 	 2.26 	 2.24 
 2.19 	 2.17 	 2.16 

 As N increases, the ratio seems to be converging asymptotically to two (the visualization in Excel 
 is clear, if you graph these three columns). But it is still greater than 2. This means that the
 growth is something larger than N, but certainly smaller than N^2.
 
 It is a reasonable estimate to approximate the growth as ~N Log N
 
 Here is the table:
 
N		Log N	M#Comp	N Log N	  Ratio
4			2	4		8		  2
8			3	14		24		  1.714285714
16			4	43		64		  1.488372093
32			5	119		160		  1.344537815
64			6	298		384		  1.288590604
128			7	731		896		  1.225718194
256			8	1720	2048	  1.190697674
512			9	3936	4608	  1.170731707
1024		10	8907	10240	  1.149657573
2048		11	19913	22528	  1.131321247
4096		12	43944	49152	  1.118514473
8192		13	96057	106496	  1.108675058
 
In the above table, this asymptotically approaches 1, which provides further evidence that order of growth
is N log N.

So: Insertion/Selection are ~N^2 
And: Merge/Q/QA are ~N log N

2. Data Type Exercise

See solution. The Java code had some tricks with Generics that were a bit unexpected, but hopefully
you were able to work around them.  Once you recognize the need to keep the linked lists in sorted
order, the rest falls into place. Note that the JUnit test case doesn't actually test all the 
special cases (indeed, I found a serious defect even after I had passed all test cases). This test
case only covers 82.7% of the UniqueBag code -- there are special cases deep within Intersects and 
Union that are still awaiting some test cases. But no matter.

The trick in developing this benchmark was being able to create very large UniqueBag objects. As long
as you add the items in reverse order, they will always take constant time to insert! Then the remaining
operations can take as long as you might need. let's review the output. These are:
 
size,identical,toArray,contains,remove,add,intersects,union

4		0	0.0		0.0		0.001	0.0		0.0		0.0		0.0
8		0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.0
16		0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.0
32		0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.0
64		0.0	0.0		0.0		0.001	0.0		0.0		0.0		0.0
128		0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.0
256		0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.0
512		0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.0
1024	0.0	0.0		0.001	0.0		0.0		0.0		0.0		0.0
2048	0.0	0.0		0.0		0.0		0.0		0.0		0.001	0.0
4096	0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.001
8192	0.0	0.0		0.0		0.0		0.0		0.0		0.0		0.0
16384	0.0	0.0		0.0		0.0		0.0		0.0		0.001	0.0
32768	0.0	0.0		0.0		0.0		0.0		0.001	0.0		0.0
65536	0.0	0.001	0.0		0.001	0.0		0.0		0.001	0.001
131072	0.0	0.001	0.001	0.001	0.0		0.0		0.041	0.002
262144	0.0	0.002	0.003	0.002	0.001	0.001	0.003	0.046
524288	0.0	0.005	0.004	0.004	0.002	0.002	0.007	0.086
1048576	0.0	0.008	0.009	0.008	0.005	0.004	0.21	0.019
2097152	0.0	0.017	0.017	0.016	0.009	0.008	0.407	0.041

Aside from some noise, you can see that size() always takes 0.0, which is ~1 because it is constant

All other operations appear to double (nearly exactly) with each doubling of the problem size. Note 
that these doublings are much more accurate than for the sorting exercise, which leads me to the
claim that the operations perform in time directly proportionate to the size of the problem. In this
case N was for the size of 'one' and M=N/2 was the size of 'two'. In any event, the ~ M+N is ~3N/2
which doubles with each doubling of N. So my Tilde approximation is

size: ~1
identical ~N (since if different returns immediately)
toArray ~N
contains ~N
add ~N
remove ~N

and I would make 

intersects ~M+N
union ~M+N

Q3. Heap Exercise.

Once you have instrumented the MaxPQ, you can run HeapExercise for the following. Note in this
case, I show the (low,high) but for your solutions you only need to show the HIGH. As you can see,
the maximum number of comparisons during insert is indeed Log N while the maximum number for 
DelMax is always less than 2*Log N or ~2 Log N. A precise answer would be ~ 2*(Log N-1) and 
both are acceptable.

Heap Trials
N	DelMax	Insert			LOG N
4		2-2		1-2			2
8		4-4		1-3			3
16		4-6		1-4			4
32		6-8		1-5			5
64		6-10	1-6			6
128		8-12	1-7			7
256		10-14	1-8			8
512		10-16	1-9			9
1024	10-18	1-10		10
2048	12-20	1-11		11
4096	14-22	1-12		12
8192	16-24	1-13		13

So the statistics supports these propositions

Q4. Three-Way Merge

This requires some tricky bit of code to make sure you cover all of the special cases in the 
proper order; be inspired by the two-way merge to make this happen.

The key is to make sure that sort calls merge (a, int lo, int left, int right, int hi) with 
distinct values.

2. You are being asked to compute the number of comparisons, C(n).

 static void sort (Comparable[] a, int lo, int hi) {
    	if (hi <= lo) return;
    	
    	int third = (hi - lo + 1)/3;
    	sort(a, lo, lo + third);
		sort(a, lo +third+1, lo + 2*third);
		sort(a, lo +2*third+1, hi);
		merge(a, lo , lo+third, lo+2*third, hi);
    }

Reviewing the sort logic, you can see that C(N) = C(N/3) + C(N/3) + C(N/3) + M(N)
where M(N) is the number of comparisons within the Merge. Assuming that N is a power 
of 3, we can eliminate much of the special cases, and simply treat this as:

  C(3) = 3*C(N/3) + M(N)
  
We need to think about the best case and the worst case for the # of comparisons in the merge.
Reviewing my code, in the BEST case, you drain one of the three subranges containing N/3 elements.
In doing so, you will see that you need two comparisons, once done, then you drain the second of
the N/3 elements; this only takes one comparison for each element. That is it. So, in best case:

  2*N/3 + 1*N/3 which is N
  
Note this is different from 2-way merge sort. Now what about worst case. Here, you don't drain any
of the three subarrays, and you are left with 1 element in each as a last spot. How long can you
prolong the need to check two comparisons within the for loop? (N/3-1) + (N/3-1) + (N/3-1). That is,
as long as there is a single element remaining in each, you need two comparisons. The above sums 
up as N-3. So this accounts for 2*(N-3) comparisons. So there are three elements left. You need 
two comparisons to reduce to two elements. Then 1 more comparison and you are done. The total
is 2*(N-3) + 2 + 1 or 2*N-3.

So in worst case, C(N) = 3*C(N/3) + (2*N-3).

Let's do some telescoping terms, and assume that N=3^n or a power of three

C(N) = 3*C(N/3) + (2*N-3)
C(N) = 3*[C(N/9) + 2*(N/3)-3] + (2*N-3)
C(N) = 3*[3*[3*C(N/27) + 2*(N/9)-3] + 2*(N/3)-3] + (2*N-3)
C(N) = 3^k*C(N/3^k) + (2*N-27) + (2*N-9) + (2*N-3)      where k=3

In general then, as k approaches n, we get:

C(N) = 3^n*C(N/3^n) + log_3(N)*(2*N) - SUM(3^k) for K=1 to n

with a little bit of mathematical prodding, it is a fundamental truth of polynomials that
x + x^2 + x^3 + x^4 + ... + x^n = (x^(n+1)-1)/(x-1) [http://mikestoolbox.com/powersum.html]

which means

C(N) = 3^n*C(1) + log_3(N)*2*N - [3^(k+1)-1]/(3-1)

and since C(1)= 0

C(N) = 2*N*log_3(N) - [3*N-1]/2

In reviewing Tilde approximation, the N*Log_3(N) term dominates the 3*N term, so we have:

C(N) = ~ 2*N*Log_3(N)

I will also accept just C(N) = ~N * Log_3(N)

and the order of growth is classified as N*Log_3(N)

Regarding A(n):

This asks for the number of array access (like on p. 275)

Arrays are accessed within merge. Each merge uses:

  2N for the copy
  2N for the move back
  
The issue is how to count the comparisons in the IF statement. 

In the worst case, you might think it would require FOUR array accesses, but as you
can see, I can cut this down to THREE by caching the lookup of aux[j] and aux[i].

  3N for compares in the worst case,
  
How many times will merge be called? You can keep dividing N by 3 no more than log_3(N) times, so: 

A(N) = 7N*log_3(N) array access in worst case.

Note to TA: willing to accept 8N*log_3(N) for full credit as well.



